// Owlby-api/lib/api-logger.ts
import { createClient } from '@supabase/supabase-js';

// Re-using the Supabase client from the web project for consistency
// This assumes that environment variables SUPABASE_URL and SUPABASE_ANON_KEY are available
const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_ANON_KEY!
);

interface APILogData {
  route: 'chat' | 'lesson' | 'story';
  userId?: string;
  chatId?: string;
  gradeLevel: number;
  model: string;
  inputText: string;
  outputText?: string;
  responseTimeMs: number;
  success: boolean;
  error?: string;
  geminiUsageMetadata?: {
    promptTokenCount: number;
    candidatesTokenCount: number;
    thinkingTokenCount?: number; // Only present for models with thinking (e.g., Gemini 2.5 Pro)
    totalTokenCount: number;
  };
}

interface LogEntry {
  // id is generated by Supabase
  timestamp: string;
  route: string;
  user_id?: string;
  session_id?: string;
  chat_id?: string;
  grade_level: number;
  model: string;
  input_tokens: number;
  output_tokens: number;
  thinking_tokens: number;
  total_tokens: number;
  input_length: number;
  output_length: number;
  response_time_ms: number;
  success: boolean;
  error_type?: string;
  input_cost: number;
  output_cost: number;
  thinking_cost: number;
  total_cost: number;
}

class APILoggingService {
  private buffer: LogEntry[] = [];
  private readonly BATCH_SIZE = 50;
  private readonly FLUSH_INTERVAL = 30000; // 30 seconds
  private flushTimer: NodeJS.Timeout;
  
  // Running cost tracking for this session
  private sessionStats = {
    totalCost: 0,
    totalInputTokens: 0,
    totalOutputTokens: 0,
    totalThinkingTokens: 0,
    totalCalls: 0,
    startTime: new Date()
  };

  // Gemini pricing per 1M tokens (update as needed)
  // Note: Thinking tokens are charged at the same rate as input tokens
  private readonly PRICING = {
    'gemini-3-flash-preview': { input: 0.50, output: 3.00, thinking: 3.00}, // Preview model, similar to Flash pricing
    'gemini-3-flash': { input: 0.5, output: 3.00, thinking: 3.00 }, // Flash has no thinking, but set for consistency
    'gemini-2.5-flash': { input: 0.30, output: 2.50, thinking: 2.50}, // Flash has no thinking, but set for consistency
    'gemini-2.5-pro': { input: 1.25, output: 10.00, thinking: 10.0 }, // Thinking charged at input rate
  };
  
  constructor() {
    this.flushTimer = setInterval(() => this.flushToSupabase(), this.FLUSH_INTERVAL);
  }

  async flushNow(): Promise<void> {
    await this.flushToSupabase();
  }

  async logAPICall(data: APILogData): Promise<void> {
    try {
      const modelName = data.model as keyof typeof this.PRICING;
      
      // Extract token counts from Gemini API response
      // promptTokenCount = input tokens (prompt + system instruction)
      // candidatesTokenCount = output tokens (response only, excludes thinking)
      // thinkingTokenCount = thinking tokens (only for models with thinking, like Gemini 2.5 Pro)
      // totalTokenCount = sum of all tokens
      const inputTokens = data.geminiUsageMetadata?.promptTokenCount || 0;
      const outputTokens = data.geminiUsageMetadata?.candidatesTokenCount || 0; // Response only, excludes thinking
      const thinkingTokens = data.geminiUsageMetadata?.thinkingTokenCount || 0; // Only for models with thinking
      
      // Calculate total tokens: input + output + thinking
      // Note: Gemini's totalTokenCount should equal this, but we calculate it ourselves for verification
      const calculatedTotalTokens = inputTokens + outputTokens + thinkingTokens;
      const reportedTotalTokens = data.geminiUsageMetadata?.totalTokenCount || calculatedTotalTokens;
      
      // Use calculated total (input + output + thinking) for consistency
      // Verify against reported total for debugging
      const totalTokens = calculatedTotalTokens;
      if (calculatedTotalTokens !== reportedTotalTokens && reportedTotalTokens > 0) {
        console.warn(`[API LOGGER] Token count mismatch: calculated=${calculatedTotalTokens}, reported=${reportedTotalTokens}`);
      }
      
      const { inputCost, outputCost, thinkingCost, totalCost } = this.calculateCostBreakdown(
        inputTokens, 
        outputTokens, 
        thinkingTokens,
        modelName
      );
      
      // DETAILED TOKEN USAGE LOG - Always visible in Vercel logs 
      console.info(`ðŸ’° [${data.route.toUpperCase()}] TOKEN USAGE & COST BREAKDOWN:`, {
        timestamp: new Date().toISOString(),
        route: data.route,
        model: data.model,
        user_id: data.userId?.substring(0, 8) + '...',
        chat_id: data.chatId,
        grade_level: data.gradeLevel,
        success: data.success,
        error: data.error || 'none',
        response_time_ms: data.responseTimeMs,
        
        // TOKEN BREAKDOWN
        tokens: {
          input_tokens: inputTokens,
          output_tokens: outputTokens, // Response only, excludes thinking
          thinking_tokens: thinkingTokens, // Only for models with thinking
          total_tokens: totalTokens,
          calculated_total: calculatedTotalTokens,
          reported_total: reportedTotalTokens,
          token_ratio: outputTokens > 0 ? (outputTokens / inputTokens).toFixed(2) : 'N/A'
        },
        
        // TEXT LENGTH ANALYSIS
        text_analysis: {
          input_chars: data.inputText.length,
          output_chars: data.outputText?.length || 0,
          chars_per_input_token: inputTokens > 0 ? (data.inputText.length / inputTokens).toFixed(2) : 'N/A',
          chars_per_output_token: outputTokens > 0 ? ((data.outputText?.length || 0) / outputTokens).toFixed(2) : 'N/A'
        },
        
        // COST BREAKDOWN
        cost_breakdown: {
          input_cost_usd: inputCost.toFixed(6),
          output_cost_usd: outputCost.toFixed(6),
          thinking_cost_usd: thinkingCost.toFixed(6),
          total_cost_usd: totalCost.toFixed(6),
          cost_per_1k_tokens: totalTokens > 0 ? ((totalCost / totalTokens) * 1000).toFixed(6) : 'N/A'
        },
        
        // PRICING RATES USED
        pricing_rates: {
          model: modelName,
          input_rate_per_1m: `$${this.PRICING[modelName].input}`,
          output_rate_per_1m: `$${this.PRICING[modelName].output}`,
          output_multiplier: `${(this.PRICING[modelName].output / this.PRICING[modelName].input).toFixed(1)}x more than input`
        },
        
        // RAW GEMINI METADATA
        raw_gemini_metadata: data.geminiUsageMetadata
      });
      
      // Update session running totals
      this.sessionStats.totalCost += totalCost;
      this.sessionStats.totalInputTokens += inputTokens;
      this.sessionStats.totalOutputTokens += outputTokens;
      this.sessionStats.totalThinkingTokens += thinkingTokens;
      this.sessionStats.totalCalls += 1;
      
      // Additional high-level summary for easy scanning
      const costInCents = totalCost * 100;
      const sessionCostInCents = this.sessionStats.totalCost * 100;
      const thinkingDisplay = thinkingTokens > 0 ? `+${thinkingTokens} thinking` : '';
      console.info(`ðŸ’¸ [${data.route.toUpperCase()}] COST SUMMARY: $${totalCost.toFixed(6)} (${costInCents.toFixed(3)}Â¢) | ${inputTokens}â†’${outputTokens}${thinkingDisplay} tokens | ${data.responseTimeMs}ms`);
      console.info(`ðŸ“Š [SESSION TOTALS] Calls: ${this.sessionStats.totalCalls} | Total Cost: $${this.sessionStats.totalCost.toFixed(6)} (${sessionCostInCents.toFixed(3)}Â¢) | Tokens: ${this.sessionStats.totalInputTokens}â†’${this.sessionStats.totalOutputTokens}+${this.sessionStats.totalThinkingTokens} thinking`);
      
      const entry: LogEntry = {
        timestamp: new Date().toISOString(),
        route: data.route,
        user_id: data.userId,
        session_id: undefined,
        chat_id: data.chatId,
        grade_level: data.gradeLevel,
        model: data.model,
        input_tokens: inputTokens,
        output_tokens: outputTokens, // Response only, excludes thinking
        thinking_tokens: thinkingTokens, // Only for models with thinking
        total_tokens: totalTokens, // input + output + thinking
        input_length: data.inputText.length,
        output_length: data.outputText?.length || 0,
        response_time_ms: data.responseTimeMs,
        success: data.success,
        error_type: data.error,
        input_cost: inputCost,
        output_cost: outputCost,
        thinking_cost: thinkingCost, // Cost for thinking tokens
        total_cost: totalCost // input_cost + output_cost + thinking_cost
      };

      this.buffer.push(entry);

      if (this.buffer.length >= this.BATCH_SIZE) {
        await this.flushToSupabase();
      }
    } catch (error) {
      console.error('[API LOGGER] Failed to log API call:', error, data);
      // Logging failures shouldn't break the API
    }
  }

  private calculateCostBreakdown(
    inputTokens: number, 
    outputTokens: number, 
    thinkingTokens: number,
    model: keyof typeof this.PRICING
  ) {
    const rates = this.PRICING[model] || this.PRICING['gemini-3-flash']; // Fallback to Flash pricing
    const inputCost = (inputTokens / 1_000_000) * rates.input;
    const outputCost = (outputTokens / 1_000_000) * rates.output;
    // Thinking tokens are charged at the same rate as input tokens
    const thinkingCost = (thinkingTokens / 1_000_000) * rates.thinking;
    const totalCost = inputCost + outputCost + thinkingCost;
    return { inputCost, outputCost, thinkingCost, totalCost };
  }

  private async flushToSupabase(): Promise<void> {
    if (this.buffer.length === 0) return;

    const batch = [...this.buffer];
    this.buffer = [];

    try {
      const { error, data } = await supabase.from('api_usage_logs').insert(batch);

      if (error) {
        console.error('[API LOGGER] Supabase insert error:', error, { batch });
        // Re-add to buffer for retry on next flush
        this.buffer.unshift(...batch);
      } else {
        console.info(`ðŸ“Š Logged ${batch.length} API calls to database`);
      }
    } catch (error) {
      console.error('[API LOGGER] Failed to flush logs to Supabase:', error, { batch });
      this.buffer.unshift(...batch);
    }
  }

  destroy(): void {
    if (this.flushTimer) {
      clearInterval(this.flushTimer);
    }
    this.flushToSupabase(); // Final flush before exit
  }
}

export const apiLogger = new APILoggingService();
export const flushApiLogger = () => apiLogger.flushNow();


// Route-specific helpers for convenience

export const logChatCall = (data: {
    userId?: string;
    chatId: string;
    gradeLevel: number;
    message: string;
    responseText?: string;
    responseTimeMs: number;
    success: boolean;
    error?: string;
    usageMetadata?: any;
    model: string;
  }) => {
    return apiLogger.logAPICall({
      route: 'chat',
      inputText: data.message,
      outputText: data.responseText,
      geminiUsageMetadata: data.usageMetadata,
      ...data
    });
  };
  
  export const logLessonCall = (data: {
    userId?: string;
    gradeLevel: number;
    topic: string;
    responseText?: string;
    responseTimeMs: number;
    success: boolean;
    error?: string;
    usageMetadata?: any;
    model: string;
  }) => {
    return apiLogger.logAPICall({
      route: 'lesson',
      inputText: data.topic,
      outputText: data.responseText,
      geminiUsageMetadata: data.usageMetadata,
      ...data
    });
  };
  
  export const logStoryCall = (data: {
    userId?: string;
    gradeLevel: number;
    prompt: string;
    responseText?: string;
    responseTimeMs: number;
    success: boolean;
    error?: string;
    usageMetadata?: any;
    model: string;
  }) => {
    return apiLogger.logAPICall({
      route: 'story',
      inputText: data.prompt,
      outputText: data.responseText,
      geminiUsageMetadata: data.usageMetadata,
      ...data
    });
  }; 